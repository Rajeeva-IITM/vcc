{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.polars  # type: ignore\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "paths = dotenv_values()\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(paths[\"DATA_DIR\"])\n",
    "\n",
    "counts = pl.read_parquet(data_path / \"processed-data/training_data-counts_uint.parquet\")\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = counts.select(cs.numeric()).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = (X - X.mean(axis=1)[:, np.newaxis]) / X.std(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=512, svd_solver=\"randomized\")\n",
    "pca.fit(X_std.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = pca.transform(X_std.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(512), np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df = pl.DataFrame(\n",
    "    data=transformed, schema=[f\"latent_{i}\" for i in range(512)]\n",
    ").with_columns(gene_name=pl.Series(counts.select(cs.numeric()).columns))\n",
    "embedding_df = embedding_df.select(\"gene_name\", cs.numeric())\n",
    "embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.write_parquet(data_path / \"gene_embeddings/PCA-train_expression.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = np.cov(X_std.T)\n",
    "covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import MiniBatchSparsePCA\n",
    "\n",
    "sparse_pca = MiniBatchSparsePCA(n_components=512, n_jobs=8, batch_size=100)\n",
    "transformed_sparse_pca = sparse_pca.fit_transform(X_std.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_sparse_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_pca_embedding_df = pl.DataFrame(\n",
    "    data=transformed_sparse_pca, schema=[f\"latent_{i}\" for i in range(512)]\n",
    ").with_columns(gene_name=pl.Series(counts.select(cs.numeric()).columns))\n",
    "sparse_pca_embedding_df = sparse_pca_embedding_df.select(\"gene_name\", cs.numeric())\n",
    "sparse_pca_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_pca_embedding_df.write_parquet(\n",
    "    data_path / \"gene_embeddings/SparsePCA-train_expression.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Checking these embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(data_path / \"gene_embeddings/PCA-train_expression.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = df.select(cs.numeric()).to_torch()\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_pca = F.normalize(X_pca, dim=-1)\n",
    "X_norm_pca.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sim = torch.matmul(X_norm_pca, X_norm_pca.T)\n",
    "gene_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.select([\"SAMD11\", \"NOC2L\", \"KLHL17\"]).with_columns(pl.all().log1p()).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([[1, 2], [2, 1]]) * torch.Tensor([1, 2]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([1, 2]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "# VAE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Embeddings\n",
    "\n",
    "from lightning.fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric = Fabric(accelerator=\"cuda\", devices=[3])\n",
    "fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneVAE(nn.Module):\n",
    "    def __init__(self, n_samples, latent_dim=256, dropout_rate=0.1):\n",
    "        super(GeneVAE, self).__init__()\n",
    "        self.n_samples = n_samples\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder layers\n",
    "        self.encoder_fc1 = nn.Linear(n_samples, 1024)\n",
    "        self.encoder_dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.encoder_fc2 = nn.Linear(1024, 512)\n",
    "        self.encoder_dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.encoder_fc3 = nn.Linear(512, 256)\n",
    "\n",
    "        # Latent space\n",
    "        self.encoder_mean = nn.Linear(256, latent_dim)\n",
    "        self.encoder_logvar = nn.Linear(256, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.decoder_fc1 = nn.Linear(latent_dim, 256)\n",
    "        self.decoder_dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.decoder_fc2 = nn.Linear(256, 512)\n",
    "        self.decoder_dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.decoder_fc3 = nn.Linear(512, 1024)\n",
    "        self.decoder_output = nn.Linear(1024, n_samples)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.gelu(self.encoder_fc1(x))\n",
    "        h1 = self.encoder_dropout1(h1)\n",
    "        h2 = F.gelu(self.encoder_fc2(h1))\n",
    "        h2 = self.encoder_dropout2(h2)\n",
    "        h3 = F.gelu(self.encoder_fc3(h2))\n",
    "\n",
    "        mean = self.encoder_mean(h3)\n",
    "        logvar = self.encoder_logvar(h3)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mean + eps * std\n",
    "        else:\n",
    "            return mean\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.gelu(self.decoder_fc1(z))\n",
    "        h3 = self.decoder_dropout1(h3)\n",
    "        h4 = F.gelu(self.decoder_fc2(h3))\n",
    "        h4 = self.decoder_dropout2(h4)\n",
    "        h5 = F.gelu(self.decoder_fc3(h4))\n",
    "        return self.decoder_output(h5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        decoded = self.decode(z)\n",
    "        return decoded, mean, logvar\n",
    "\n",
    "    def get_gene_embeddings(self, x):\n",
    "        \"\"\"Extract gene embeddings (means from latent space)\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            mean, _ = self.encode(x)\n",
    "        return mean\n",
    "\n",
    "\n",
    "def vae_loss(recon_x, x, mean, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss with β parameter for controlling regularization\n",
    "    Higher β = more regularization, more structured latent space\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (MSE for continuous data)\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
    "\n",
    "    # KL Divergence loss\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + beta * kld, recon_loss, kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = counts.select(cs.numeric().log1p()).to_torch(\"tensor\").T.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GeneVAE(counts.shape[0])\n",
    "# model.compile()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "dataloader = fabric.setup_dataloaders(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Epochs\"):\n",
    "    for batch_idx, (batch_data,) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_batch, mean, logvar = model(batch_data)\n",
    "        loss, recon_loss, kld_loss = vae_loss(recon_batch, batch_data, mean, logvar, beta=1.0)\n",
    "\n",
    "        fabric.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "    if epoch % 5 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch:3d}/{n_epochs}, Loss: {loss:.4f}, \"\n",
    "            f\"Recon: {recon_loss:.4f}, KLD: {kld_loss:.4f}\"\n",
    "        )\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(dataset[:][0])[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.corrcoef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.corrcoef()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_norm = F.normalize(embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(embeddings_norm, embeddings_norm.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "# Quantile based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = X.quantile(\n",
    "    torch.linspace(0, 1, 256),\n",
    "    dim=1,\n",
    "    # keepdim=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_norm = F.normalize(quantiles.T, dim=1)\n",
    "quantiles_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = torch.matmul(quantiles_norm, quantiles_norm.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = F.normalize(X, dim=-1)\n",
    "torch.matmul(X_norm, X_norm.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.normalize(X, 1, dim=-1).sum(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.corrcoef(torch.argsort(X, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.corrcoef(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_df = pl.DataFrame(\n",
    "    data=quantiles.T, schema=[f\"quantile_{i}\" for i in range(256)]\n",
    ").with_columns(gene_name=pl.Series(counts.select(cs.numeric()).columns))\n",
    "quantiles_df = quantiles_df.select(\"gene_name\", cs.numeric())\n",
    "quantiles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_df.select(cs.numeric()).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles_df.write_parquet(data_path / \"gene_embeddings/quantiles-train_expression.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
