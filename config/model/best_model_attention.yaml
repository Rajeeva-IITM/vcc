# This was froma an optuna run

_target_: src.models.vcc_lightning.VCCModule
net:
  _target_: src.models.components.vcc_model_attention.CellModelAttention
  ko_processor_args:
    input_size: 512
    hidden_layers: [256, 128, 64, 32, 16, 8]
    output_size: 256
    dropout: 0.11384507428099798
    activation: silu
    # no_processing: true
  exp_processor_args:
    input_size: 18080
    hidden_layers: [1024,512, 256, 128, 64, 32, 16, 8]
    output_size: 256
    dropout: 0.7989976393418468
    activation: silu
    # no_processing: true
  attention_args:
    embed_dim: 256 # Must be same as the dimensions of your key and value
    num_heads: 1
  decoder_args:
    input_size: 256
    hidden_layers: [4,8,16,32,64,128,256]
    output_size: ${model.net.exp_processor_args.input_size}
    dropout: 0.22478678235543365
    activation: silu
  fusion_type: 'cross_attn'
loss_fn:
  _target_: src.models.components.loss_functions.LogCoshError
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 9.648602028194232e-05
  weight_decay: 1e-5
scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  _partial_: true
  max_lr: 0.003240143620377953

# lr: 1e-5
# max_lr: 1e-2
# weight_decay: 1e-5
# composite_loss_lambda: 1
